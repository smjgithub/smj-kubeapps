apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ template "elasticsearch.fullname" . }}
  labels:
    app: {{ template "elasticsearch.fullname" . }}
    chart: "{{ .Chart.Name }}-{{ .Chart.Version }}"
    release: "{{ .Release.Name }}"
    heritage: "{{ .Release.Service }}"
data:
  elasticsearch.yml: |-
    cluster.name: {{ .Values.cluster.name }}

    node:
      name: ${HOSTNAME}
      master: ${NODE_MASTER:false}
      data: ${NODE_DATA:false}
      ingest: ${NODE_INGEST:false}
      ml: ${NODE_ML:false}

    max_local_storage_nodes: ${MAX_LOCAL_STORAGE_NODES:1}

    search.remote.connect: ${SEARCH_REMOTE_CONNECT:false}

    network.host: ${NETWORK_HOST:0.0.0.0}

    path:
      data: ${PATH_DATA}
      logs: ${PATH_LOG}

    # see https://github.com/kubernetes/kubernetes/issues/3595
    bootstrap.memory_lock: ${BOOTSTRAP_MEMORY_LOCK:false}

    http:
      enabled: ${HTTP_ENABLE}
      compression: true
      cors:
        enabled: ${HTTP_CORS_ENABLE}
        allow-origin: ${HTTP_CORS_ALLOW_ORIGIN}

    discovery:
      zen:
        ping.unicast.hosts: ${DISCOVERY_SERVICE:}
        minimum_master_nodes: ${MINIMUM_MASTER_NODES:2}

{{- if .Values.cluster.xpackEnable }}
    xpack.monitoring.collection.enabled: ${XPACK_MONITORING_ENABLED:false}
{{- end }}

    # see https://github.com/elastic/elasticsearch-definitive-guide/pull/679
    processors: ${PROCESSORS:}

    # avoid split-brain w/ a minimum consensus of two masters plus a data node
    gateway.expected_master_nodes: ${EXPECTED_MASTER_NODES:2}
    gateway.expected_data_nodes: ${EXPECTED_DATA_NODES:1}
    gateway.recover_after_time: ${RECOVER_AFTER_TIME:5m}
    gateway.recover_after_master_nodes: ${RECOVER_AFTER_MASTER_NODES:2}
    gateway.recover_after_data_nodes: ${RECOVER_AFTER_DATA_NODES:1}
{{- with .Values.cluster.config }}
{{ toYaml . | indent 4 }}
{{- end }}
#  log4j2.properties: |-
#    status = error
#    appender.console.type = Console
#    appender.console.name = console
#    appender.console.layout.type = PatternLayout
#    appender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] %marker%m%n
#    rootLogger.level = info
#    rootLogger.appenderRef.console.ref = console
#    logger.searchguard.name = com.floragunn
#    logger.searchguard.level = info
#{{- if .Values.data.hooks.drain.enabled }}
#  pre-stop-hook.sh: |-
#    #!/bin/bash
#    exec &> >(tee -a "/var/log/elasticsearch-hooks.log")
#    NODE_NAME=${HOSTNAME}
#    echo "Prepare to migrate data of the node ${NODE_NAME}"
#    echo "Move all data from node ${NODE_NAME}"
#    curl -s -XPUT -H 'Content-Type: application/json' '{{ template "elasticsearch.query.fullname" . }}:9200/_cluster/settings' -d "{
#      \"transient\" :{
#          \"cluster.routing.allocation.exclude._name\" : \"${NODE_NAME}\"
#      }
#    }"
#    echo ""
#
#    while true ; do
#      echo -e "Wait for node ${NODE_NAME} to become empty"
#      SHARDS_ALLOCATION=$(curl -s -XGET 'http://{{ template "elasticsearch.query.fullname" . }}:9200/_cat/shards')
#      if ! echo "${SHARDS_ALLOCATION}" | grep -E "${NODE_NAME}"; then
#        break
#      fi
#      sleep 1
#    done
#    echo "Node ${NODE_NAME} is ready to shutdown"
#  post-start-hook.sh: |-
#    #!/bin/bash
#    exec &> >(tee -a "/var/log/elasticsearch-hooks.log")
#    NODE_NAME=${HOSTNAME}
#    CLUSTER_SETTINGS=$(curl -s -XGET "http://{{ template "elasticsearch.query.fullname" . }}:9200/_cluster/settings")
#    if echo "${CLUSTER_SETTINGS}" | grep -E "${NODE_NAME}"; then
#      echo "Activate node ${NODE_NAME}"
#      curl -s -XPUT -H 'Content-Type: application/json' "http://{{ template "elasticsearch.query.fullname" . }}:9200/_cluster/settings" -d "{
#        \"transient\" :{
#            \"cluster.routing.allocation.exclude._name\" : null
#        }
#      }"
#    fi
#    echo "Node ${NODE_NAME} is ready to be used"
#{{- end }}
